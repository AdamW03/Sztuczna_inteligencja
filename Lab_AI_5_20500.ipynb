{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813b1fcf-cb77-4b0f-9c89-4935ff3cef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca20cbbf-5cf3-4019-b535-a96fbd783ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer technology is a cornerstone of electrical power systems and has been in use since its invention by Nikola Tesla in 1885 and later developed by George Westinghouse and Michael Faraday. It is a passive device that transfers electrical energy between two or more circuits through electromagnetic induction. Transformers are essential for the operation of power grids, allowing for the efficient transmission of electricity over long distances without significant energy loss.\n",
      "\n",
      "Here are some key aspects of transformer technology:\n",
      "\n",
      "1. **Principle of Operation**: The core principle of a transformer is based on Faraday's Law of Electromagnetic Induction. When an alternating current (AC) is applied to the primary winding, it induces an electromotive force (EMF) in the secondary winding, provided they are connected by a common magnetic core. This allows for the transformation of voltage levels between the two windings.\n",
      "\n",
      "2. **Types of Transformers**:\n",
      "   - **Power Transformers**: Used in power stations and substations to step voltages up (tap-changer types) or down (oscillation-type) for efficient transmission and distribution.\n",
      "   - ** Distribution Transformers**: Install at the end of a power line, stepping down voltage to safe levels suitable for household and commercial use.\n",
      "   - **Autotransformers**: Combine primary and secondary windings into one winding, sharing part of the same coil, which allows for variable voltage regulation without the need for mechanical switching devices like tap changers.\n",
      "   - **Isolation Transformers**: Provide electrical isolation between the input and output, effectively separating ground loops and galvanically isolating the two sides of the transformer.\n",
      "\n",
      "3. **Efficiency**: Transformers are highly efficient at transferring energy; losses occur due to resistance in the windings and leakage reactance, which results in some power being dissipated as heat. The efficiency of a transformer can be calculated by comparing the input (primary) power with the output (secondary) power minus any losses.\n",
      "\n",
      "4. **Energy Losses**: The primary causes of energy loss in transformers are:\n",
      "   - **Copper Losses**: Due to the resistance of the windings (I²R losses).\n",
      "   - **Magnetic Core Losses**: Caused by hysteresis and eddy currents within the core material.\n",
      "\n",
      "5. **Design Considerations**: The design of a transformer is influenced by factors such as its voltage rating, power capacity, frequency of operation, type of insulation, core size, winding configuration, and cooling method. Transformers can be air-cooled or oil-immersed (liquid-filled), which allows for the dissipation of heat generated during operation.\n",
      "\n",
      "6. **Environmental Impact**: The insulating fluid in liquid-filled transformers can pose environmental risks if it leaks. Efforts are made to use eco-friendly fluids and to properly manage transformer waste to minimize ecological impact.\n",
      "\n",
      "7. **Smart Grid Technology**: Modern transformers are becoming more intelligent with the integration of sensors and communication capabilities, allowing for remote monitoring, diagnostics, and control, which contribute to the advancement of smart grids.\n",
      "\n",
      "8. **Research and Development**: Ongoing research focuses on improving the efficiency, reliability, and capacity of transformers. This includes developing new materials for the core and coils (such as amorphous or nanocrystalline materials), advanced cooling techniques, and design innovations to handle the increasing demand for electricity in a sustainable manner.\n",
      "\n",
      "9. **Future Challenges**: As the world transitions towards renewable energy sources and with the rise of electric vehicles, the role of transformers is evolving. The integration of these technologies will require new types of transformers that can handle different power sources and manage higher levels of variability in power demand.\n",
      "\n",
      "Transformer technology continues to advance as it faces new challenges and opportunities in the context of an evolving global energy landscape. Its ability to convert voltage levels efficiently and reliably remains indispensable for the transmission and distribution of electrical power.\n"
     ]
    }
   ],
   "source": [
    "message = \"Tell me something about transformer technology\"\n",
    "#response = ollama.chat(model='llama3.2', messages=[{\"role\":\"user\", \"content\":message}])\n",
    "#response = ollama.chat(model='deepseek-r1:14b', messages=[{\"role\":\"user\", \"content\":message}])\n",
    "response = ollama.chat(model='wizardlm2:7b', messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30eb329b-1abc-4506-810a-1ace556e7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niektóre strony internetowe wymagają użycia odpowiednich nagłówków podczas ich pobierania:\n",
    "headers = {\n",
    "\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Klasa reprezentująca stronę internetową, która zostanie pobeana do podsumowania\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        # Użyjemy bibliotek BeautifulSoup do zbudowania biektu z treścią\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\", \"link\", \"ul\", \"li\", \"a\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9afe7b1-6e85-4dc9-b989-cda1db513bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Duży model językowy – Wikipedia, wolna encyklopedia\n",
      "text:  Menu główne\n",
      "Menu główne\n",
      "przypnij\n",
      "ukryj\n",
      "Nawigacja\n",
      "Dla czytelników\n",
      "Dla wikipedystów\n",
      "Szukaj\n",
      "Wygląd\n",
      "Narzędzia osobiste\n",
      "Strony dla anonimowych edytorów\n",
      "Spis treści\n",
      "przypnij\n",
      "ukryj\n",
      "Przełącz stan spisu treści\n",
      "Duży model językowy\n",
      "55 języków\n",
      "polski\n",
      "Narzędzia\n",
      "Narzędzia\n",
      "przypnij\n",
      "ukryj\n",
      "Działania\n",
      "Ogólne\n",
      "Drukuj lub eksportuj\n",
      "W innych projektach\n",
      "Wygląd\n",
      "przypnij\n",
      "ukryj\n",
      "Z Wikipedii, wolnej encyklopedii\n",
      "Duży model językowy\n",
      "(ang.\n",
      "large language model\n",
      ",\n",
      "LLM\n",
      ")\n",
      "– model\n",
      "umożliwiający generowanie tekstu oraz realizację zadań związanych z\n",
      ". Modele\n",
      "LLM\n",
      "są szkolone w ramach\n",
      "lub słabo nadzorowanego\n",
      "z wykorzystaniem dużych ilości danych tekstowych. Proces ten jest intensywny obliczeniowo\n",
      ". Duże modele językowe mogą być wykorzystywane do generowana tekstu poprzez wielokrotne przewidywanie następnego tokenu lub słowa, przez co zaliczane są do\n",
      ".\n",
      "Duże modele językowe są\n",
      ". Największe i najbardziej zdolne modele językowe oparte są na architekturze\n",
      ".\n",
      "Przykładami dużych modeli językowych są modele z serii\n",
      "zbudowane przez\n",
      "(np.\n",
      "﻿\n",
      ",\n",
      "﻿\n",
      "), używane w chatbotach\n",
      "i Microsoft Copilot, a także modele\n",
      "zbudowane przez\n",
      ". Istnieją również chińskie modele jak\n",
      "czy polskie jak\n",
      "i\n",
      ".\n",
      "Historia\n",
      "[\n",
      "|\n",
      "]\n",
      "Przed rokiem 2017 istniało kilka modeli językowych, które — jak na ówczesne możliwości — były uważane za duże. W latach 90.\n",
      "pozwoliły na rozwinięcie się metod statystycznego modelowania języka. W 2001 roku model bazujący na\n",
      "został wytrenowany na 300 milionach słów\n",
      ". W latach dwutysięcznych, wraz z popularyzacją\n",
      ", rozpoczęto tworzenie zbiorów językowych o skali porównywalnej z Internetem\n",
      ", na bazie których podjęto próby uczenia statystycznych modeli językowych\n",
      ".\n",
      "Po tym, jak sieci neuronowe stały się popularne w przetwarzaniu obrazów około 2012 roku\n",
      ", zaczęto je również stosować do przetwarzania tekstu. W 2016 roku Google wprowadziło w\n",
      "swój model językowy oparty na\n",
      ".\n",
      "Wykres pokazujący łączną ilość obliczeń (we\n",
      "-sach) wymaganą do wytrenowania modelu w kolejnych latach dla wybranych dużych modeli AI. Większość dużych modeli to modele językowe lub mutlimodalne ze zdolnościami przetwarzania języka.\n",
      "W 2017 roku naukowcy z Google zaproponowali architekturę\n",
      "opartą na\n",
      "opracowanym w 2014 roku\n",
      ". W 2018 roku Google wypuściło model BERT oparty wyłącznie na\n",
      ". Od 2023 roku akademickie i badawcze zainteresowanie BERT-em zaczęło stopniowo maleć na rzecz modeli opartych na\n",
      "jak\n",
      ".\n",
      "Od 2022 roku zyskują na popularności modele o otwartym kodzie źródłowym – początkowo za sprawą projektów takich jak BLOOM\n",
      "i\n",
      ", choć oba objęte są ograniczeniami dotyczącymi zakresu zastosowań. Modele\n",
      "udostępnione zostały na bardziej liberalnej licencji\n",
      ". W styczniu 2025 roku firma\n",
      "wypuściła w formie otwartego kodu model DeepSeek R1, mający 671 miliardów parametrów. Jego wydajność jest porównywalna z modelem OpenAI o1, przy znacznie niższych kosztach eksploatacji\n",
      ".\n",
      "Od 2023 roku wiele modeli ma charakter\n",
      ", co oznacza, że potrafią analizować i generować różne typy danych, takie jak tekst, obrazy czy dźwięk\n",
      ".\n",
      "W roku 2024 największe i najbardziej zaawansowane modele językowe oparte są na architekturze transformatora. Niektóre nowsze implementacje wykorzystują jednak inne podejścia, takie jak\n",
      "(RNN) czy\n",
      "﻿\n",
      ".\n",
      "Trenowanie i architektura\n",
      "[\n",
      "|\n",
      "]\n",
      "Zobacz też:\n",
      ".\n",
      "Wzmacnianie z informacją zwrotną od człowieka\n",
      "[\n",
      "|\n",
      "]\n",
      "to metoda umożliwiająca dostosowanie działania modelu językowego do ludzkich oczekiwań. Preferencje użytkowników definiuje się poprzez trenowanie tzw. modelu nagród, który następnie służy do dalszego uczenia modelu językowego z wykorzystaniem algorytmów\n",
      "takich jak\n",
      "proximal policy optimization\n",
      "(PPO)\n",
      ".\n",
      "Mieszanka ekspertów\n",
      "[\n",
      "|\n",
      "]\n",
      "Zobacz też:\n",
      ".\n",
      "Największe modele językowe bywają zbyt kosztowne w trenowaniu i bezpośrednim zastosowaniu. Dlatego coraz częściej stosuje się podejście mieszanki ekspertów (ang. Mixture of Experts, MoE)\n",
      ". MoE to technika, która dzieli przestrzeń problemu między wiele wyspecjalizowanych\n",
      "przez co inferencja aktywuje tylko te części sieci, które są najbardziej odpowiednie\n",
      ", a sama metoda jest zaliczana do metod\n",
      ".\n",
      "Dostrajanie na podstawie instrukcji\n",
      "[\n",
      "|\n",
      "]\n",
      "Duże modele językowe uczą się generować poprawne odpowiedzi i zastępować naiwne uzupełnienia dzięki kilku wstępnym korektom wprowadzonym przez człowieka oraz zastosowaniu podejścia\n",
      "self‑instruct\n",
      ". Na przykład w odpowiedzi na polecenie: „\n",
      "Napisz esej na temat głównych motywów przedstawionych w Hamlecie\n",
      "” model mógłby najpierw wygenerować: „\n",
      "Jeśli oddasz esej po 17 marca, Twoja ocena zostanie obniżona o 10% za każdy dzień opóźnienia\n",
      "”, bazując na częstotliwości takiego ciągu w\n",
      ".\n",
      "Koszt\n",
      "[\n",
      "|\n",
      "]\n",
      "Szacunkowy koszt trenowania wybranych modeli AI\n",
      "Trenowanie i działanie dużych modeli językowych zazwyczaj wymaga ogromnej\n",
      "i zużycia\n",
      ", co rodzi pytania dotyczące wpływu na\n",
      ".\n",
      "Rozumowanie\n",
      "[\n",
      "|\n",
      "]\n",
      "Pod koniec 2024 roku w rozwoju dużych modeli językowych pojawił się nowy kierunek, skoncentrowany na zadaniach wymagających złożonego\n",
      ". Modele tego typu, określane jako „modele rozumujące”, zostały wytrenowane tak, aby poświęcać więcej czasu na generowanie rozwiązań krok po kroku (ang. chain-of-thought) przed udzieleniem odpowiedzi końcowej – w sposób zbliżony do ludzkiego procesu rozwiązywania problemów\n",
      ". Trend ten zapoczątkowała firma OpenAI, wprowadzając we wrześniu 2024 roku model o1\n",
      ", a następnie model o3 w grudniu 2024\n",
      ". W porównaniu z tradycyjnymi LLM-ami, nowe modele wykazywały znaczną poprawę wyników w zadaniach z matematyki, nauk ścisłych oraz programowania. Przykładowo, na zadaniach z eliminacji do\n",
      "model GPT-4o uzyskał 13% skuteczności, podczas gdy model o1 osiągnął aż 83%\n",
      ".\n",
      "W styczniu 2025 roku chińska firma\n",
      "przedstawiła model DeepSeek‑R1 — model rozumowania o otwartych wagach, posiadający 671 miliardów parametrów, który osiągnął wyniki porównywalne do modelu o1 firmy OpenAI, przy znacznie niższych kosztach operacyjnych. W przeciwieństwie do zastrzeżonych modeli OpenAI, otwarta architektura DeepSeek‑R1 umożliwiła badaczom analizę i dalszy rozwój algorytmu, choć dane treningowe pozostały nieupublicznione\n",
      ".\n",
      "Tego typu podejście z reguły mają większe wymagania obliczeniowe w porównaniu z bezpośrednim podejściem ponieważ model musi generować wiele odpowiedzi dla każdego kroku jednak pozwala to na osiągnięcie lepszych wyników w dziedzinach wymagających myślenia domenowego\n",
      ". Aby zmniejszyć ilość występowania\n",
      ", stosowane są dodatkowe techniki jak\n",
      ",\n",
      "czy\n",
      ".\n",
      "Oddziaływanie\n",
      "[\n",
      "|\n",
      "]\n",
      "Istnieją opinie twierdzące, że nie ma możliwości rozróżnienia tekstu stworzonego przez duży model językowy i przez człowieka\n",
      ". Goldman Sachs w 2023 roku twierdził, że LLM-y są w stanie zwiększyć globalne\n",
      "o 7% w ciągu dekady i bedą potrafiły wystawić na automatyzację pracę 300 mln osób\n",
      ".\n",
      "Prawa autorskie\n",
      "[\n",
      "|\n",
      "]\n",
      "W roku 2023, do sądów w Stanach Zjednoczonych wpłynęło kilka wniosków podważających używanie danych chronionych\n",
      "do trenowania modeli językowych z obrońcami opierającymi się na instytucję\n",
      ".\n",
      "Bezpieczeństwo\n",
      "[\n",
      "|\n",
      "]\n",
      "Duże modele językowe mogą być używane do tworzenia\n",
      ", w sposób świadomy lub nie lub do innych celów\n",
      ". Dostępność dużych modeli językowych może pozwolić na obniżenie poziom umiejętności wymaganych do popełnienia czynów\n",
      ".\n",
      "Dodatkowo, istnieje możliwość osadzenia uśpionych\n",
      ", czyli ukrytych funkcjonalności, które w normalnych warunkach nie wykonują akcji, a po uzyskaniu impulsu aktywującego, rozpoczynają wykonywania szkodliwych działań\n",
      ".\n",
      "Aplikacje LLM zawierają odpowiednie filtry moderacyjne jednak nie są one w pełni efektywne i pozwalają na wykorzystanie jako\n",
      "czy różnych nielegalnych operacji\n",
      ".\n",
      "Stronniczość algorytmiczna\n",
      "[\n",
      "|\n",
      "]\n",
      "Podczas gdy duże modele językowe są w stanie generować tekst przypominający ludzki, są skłonne do dziedziczenia i powiększania stronniczości zawartej w\n",
      ". Stronniczość może się objawiać w błędnym i niesprawiedliwym traktowaniu różnych grup demograficznych\n",
      ".\n",
      "Zobacz też\n",
      "[\n",
      "|\n",
      "]\n",
      "Przypisy\n",
      "[\n",
      "|\n",
      "]\n",
      "Linki zewnętrzne\n",
      "[\n",
      "|\n",
      "]\n",
      "Pojęcia\n",
      "Aplikacje\n",
      "Implementacje\n",
      "Tekst do obrazu\n",
      "Tekst do wideo\n",
      "Inne\n",
      "Architektury\n",
      "(\n",
      "model językowy\n",
      "):\n",
      ":\n",
      "Źródło: „\n",
      "”\n",
      ":\n",
      "Ukryte kategorie:\n",
      "Szukaj\n",
      "Szukaj\n",
      "Przełącz stan spisu treści\n",
      "Duży model językowy\n",
      "55 języków\n"
     ]
    }
   ],
   "source": [
    "web = Website(\"https://pl.wikipedia.org/wiki/Duży_model_językowy\")\n",
    "print(\"Title: \", web.title)\n",
    "print(\"text: \", web.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "255b2c28-5c7a-44bd-a88d-6bb148c733c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown in Polish.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9babcd90-28a3-48e4-a24e-741d204cef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a not long summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "888f7219-44e5-4050-93fc-406df9abe8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b06c505-ce56-4ded-b206-226b22054466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant that analyzes the contents of a website and provides a short summary, ignoring text that might be navigation related. Respond in markdown in Polish.'},\n",
       " {'role': 'user',\n",
       "  'content': 'You are looking at a website titled Duży model językowy – Wikipedia, wolna encyklopedia\\nThe contents of this website is as follows; please provide a not long summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\\nMenu główne\\nMenu główne\\nprzypnij\\nukryj\\nNawigacja\\nDla czytelników\\nDla wikipedystów\\nSzukaj\\nWygląd\\nNarzędzia osobiste\\nStrony dla anonimowych edytorów\\nSpis treści\\nprzypnij\\nukryj\\nPrzełącz stan spisu treści\\nDuży model językowy\\n55 języków\\npolski\\nNarzędzia\\nNarzędzia\\nprzypnij\\nukryj\\nDziałania\\nOgólne\\nDrukuj lub eksportuj\\nW innych projektach\\nWygląd\\nprzypnij\\nukryj\\nZ Wikipedii, wolnej encyklopedii\\nDuży model językowy\\n(ang.\\nlarge language model\\n,\\nLLM\\n)\\n– model\\numożliwiający generowanie tekstu oraz realizację zadań związanych z\\n. Modele\\nLLM\\nsą szkolone w ramach\\nlub słabo nadzorowanego\\nz wykorzystaniem dużych ilości danych tekstowych. Proces ten jest intensywny obliczeniowo\\n. Duże modele językowe mogą być wykorzystywane do generowana tekstu poprzez wielokrotne przewidywanie następnego tokenu lub słowa, przez co zaliczane są do\\n.\\nDuże modele językowe są\\n. Największe i najbardziej zdolne modele językowe oparte są na architekturze\\n.\\nPrzykładami dużych modeli językowych są modele z serii\\nzbudowane przez\\n(np.\\n\\ufeff\\n,\\n\\ufeff\\n), używane w chatbotach\\ni Microsoft Copilot, a także modele\\nzbudowane przez\\n. Istnieją również chińskie modele jak\\nczy polskie jak\\ni\\n.\\nHistoria\\n[\\n|\\n]\\nPrzed rokiem 2017 istniało kilka modeli językowych, które — jak na ówczesne możliwości — były uważane za duże. W latach 90.\\npozwoliły na rozwinięcie się metod statystycznego modelowania języka. W 2001 roku model bazujący na\\nzostał wytrenowany na 300 milionach słów\\n. W latach dwutysięcznych, wraz z popularyzacją\\n, rozpoczęto tworzenie zbiorów językowych o skali porównywalnej z Internetem\\n, na bazie których podjęto próby uczenia statystycznych modeli językowych\\n.\\nPo tym, jak sieci neuronowe stały się popularne w przetwarzaniu obrazów około 2012 roku\\n, zaczęto je również stosować do przetwarzania tekstu. W 2016 roku Google wprowadziło w\\nswój model językowy oparty na\\n.\\nWykres pokazujący łączną ilość obliczeń (we\\n-sach) wymaganą do wytrenowania modelu w kolejnych latach dla wybranych dużych modeli AI. Większość dużych modeli to modele językowe lub mutlimodalne ze zdolnościami przetwarzania języka.\\nW 2017 roku naukowcy z Google zaproponowali architekturę\\nopartą na\\nopracowanym w 2014 roku\\n. W 2018 roku Google wypuściło model BERT oparty wyłącznie na\\n. Od 2023 roku akademickie i badawcze zainteresowanie BERT-em zaczęło stopniowo maleć na rzecz modeli opartych na\\njak\\n.\\nOd 2022 roku zyskują na popularności modele o otwartym kodzie źródłowym – początkowo za sprawą projektów takich jak BLOOM\\ni\\n, choć oba objęte są ograniczeniami dotyczącymi zakresu zastosowań. Modele\\nudostępnione zostały na bardziej liberalnej licencji\\n. W styczniu 2025 roku firma\\nwypuściła w formie otwartego kodu model DeepSeek R1, mający 671 miliardów parametrów. Jego wydajność jest porównywalna z modelem OpenAI o1, przy znacznie niższych kosztach eksploatacji\\n.\\nOd 2023 roku wiele modeli ma charakter\\n, co oznacza, że potrafią analizować i generować różne typy danych, takie jak tekst, obrazy czy dźwięk\\n.\\nW roku 2024 największe i najbardziej zaawansowane modele językowe oparte są na architekturze transformatora. Niektóre nowsze implementacje wykorzystują jednak inne podejścia, takie jak\\n(RNN) czy\\n\\ufeff\\n.\\nTrenowanie i architektura\\n[\\n|\\n]\\nZobacz też:\\n.\\nWzmacnianie z informacją zwrotną od człowieka\\n[\\n|\\n]\\nto metoda umożliwiająca dostosowanie działania modelu językowego do ludzkich oczekiwań. Preferencje użytkowników definiuje się poprzez trenowanie tzw. modelu nagród, który następnie służy do dalszego uczenia modelu językowego z wykorzystaniem algorytmów\\ntakich jak\\nproximal policy optimization\\n(PPO)\\n.\\nMieszanka ekspertów\\n[\\n|\\n]\\nZobacz też:\\n.\\nNajwiększe modele językowe bywają zbyt kosztowne w trenowaniu i bezpośrednim zastosowaniu. Dlatego coraz częściej stosuje się podejście mieszanki ekspertów (ang. Mixture of Experts, MoE)\\n. MoE to technika, która dzieli przestrzeń problemu między wiele wyspecjalizowanych\\nprzez co inferencja aktywuje tylko te części sieci, które są najbardziej odpowiednie\\n, a sama metoda jest zaliczana do metod\\n.\\nDostrajanie na podstawie instrukcji\\n[\\n|\\n]\\nDuże modele językowe uczą się generować poprawne odpowiedzi i zastępować naiwne uzupełnienia dzięki kilku wstępnym korektom wprowadzonym przez człowieka oraz zastosowaniu podejścia\\nself‑instruct\\n. Na przykład w odpowiedzi na polecenie: „\\nNapisz esej na temat głównych motywów przedstawionych w Hamlecie\\n” model mógłby najpierw wygenerować: „\\nJeśli oddasz esej po 17 marca, Twoja ocena zostanie obniżona o 10% za każdy dzień opóźnienia\\n”, bazując na częstotliwości takiego ciągu w\\n.\\nKoszt\\n[\\n|\\n]\\nSzacunkowy koszt trenowania wybranych modeli AI\\nTrenowanie i działanie dużych modeli językowych zazwyczaj wymaga ogromnej\\ni zużycia\\n, co rodzi pytania dotyczące wpływu na\\n.\\nRozumowanie\\n[\\n|\\n]\\nPod koniec 2024 roku w rozwoju dużych modeli językowych pojawił się nowy kierunek, skoncentrowany na zadaniach wymagających złożonego\\n. Modele tego typu, określane jako „modele rozumujące”, zostały wytrenowane tak, aby poświęcać więcej czasu na generowanie rozwiązań krok po kroku (ang. chain-of-thought) przed udzieleniem odpowiedzi końcowej – w sposób zbliżony do ludzkiego procesu rozwiązywania problemów\\n. Trend ten zapoczątkowała firma OpenAI, wprowadzając we wrześniu 2024 roku model o1\\n, a następnie model o3 w grudniu 2024\\n. W porównaniu z tradycyjnymi LLM-ami, nowe modele wykazywały znaczną poprawę wyników w zadaniach z matematyki, nauk ścisłych oraz programowania. Przykładowo, na zadaniach z eliminacji do\\nmodel GPT-4o uzyskał 13% skuteczności, podczas gdy model o1 osiągnął aż 83%\\n.\\nW styczniu 2025 roku chińska firma\\nprzedstawiła model DeepSeek‑R1 — model rozumowania o otwartych wagach, posiadający 671 miliardów parametrów, który osiągnął wyniki porównywalne do modelu o1 firmy OpenAI, przy znacznie niższych kosztach operacyjnych. W przeciwieństwie do zastrzeżonych modeli OpenAI, otwarta architektura DeepSeek‑R1 umożliwiła badaczom analizę i dalszy rozwój algorytmu, choć dane treningowe pozostały nieupublicznione\\n.\\nTego typu podejście z reguły mają większe wymagania obliczeniowe w porównaniu z bezpośrednim podejściem ponieważ model musi generować wiele odpowiedzi dla każdego kroku jednak pozwala to na osiągnięcie lepszych wyników w dziedzinach wymagających myślenia domenowego\\n. Aby zmniejszyć ilość występowania\\n, stosowane są dodatkowe techniki jak\\n,\\nczy\\n.\\nOddziaływanie\\n[\\n|\\n]\\nIstnieją opinie twierdzące, że nie ma możliwości rozróżnienia tekstu stworzonego przez duży model językowy i przez człowieka\\n. Goldman Sachs w 2023 roku twierdził, że LLM-y są w stanie zwiększyć globalne\\no 7% w ciągu dekady i bedą potrafiły wystawić na automatyzację pracę 300 mln osób\\n.\\nPrawa autorskie\\n[\\n|\\n]\\nW roku 2023, do sądów w Stanach Zjednoczonych wpłynęło kilka wniosków podważających używanie danych chronionych\\ndo trenowania modeli językowych z obrońcami opierającymi się na instytucję\\n.\\nBezpieczeństwo\\n[\\n|\\n]\\nDuże modele językowe mogą być używane do tworzenia\\n, w sposób świadomy lub nie lub do innych celów\\n. Dostępność dużych modeli językowych może pozwolić na obniżenie poziom umiejętności wymaganych do popełnienia czynów\\n.\\nDodatkowo, istnieje możliwość osadzenia uśpionych\\n, czyli ukrytych funkcjonalności, które w normalnych warunkach nie wykonują akcji, a po uzyskaniu impulsu aktywującego, rozpoczynają wykonywania szkodliwych działań\\n.\\nAplikacje LLM zawierają odpowiednie filtry moderacyjne jednak nie są one w pełni efektywne i pozwalają na wykorzystanie jako\\nczy różnych nielegalnych operacji\\n.\\nStronniczość algorytmiczna\\n[\\n|\\n]\\nPodczas gdy duże modele językowe są w stanie generować tekst przypominający ludzki, są skłonne do dziedziczenia i powiększania stronniczości zawartej w\\n. Stronniczość może się objawiać w błędnym i niesprawiedliwym traktowaniu różnych grup demograficznych\\n.\\nZobacz też\\n[\\n|\\n]\\nPrzypisy\\n[\\n|\\n]\\nLinki zewnętrzne\\n[\\n|\\n]\\nPojęcia\\nAplikacje\\nImplementacje\\nTekst do obrazu\\nTekst do wideo\\nInne\\nArchitektury\\n(\\nmodel językowy\\n):\\n:\\nŹródło: „\\n”\\n:\\nUkryte kategorie:\\nSzukaj\\nSzukaj\\nPrzełącz stan spisu treści\\nDuży model językowy\\n55 języków'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_for(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1415e61-0086-47b0-85d3-292bf204b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='llama3.2'\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2378bfd-fab4-4350-a2a2-acbec3400e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Sumary Battle.net\\n\\n### Optymalne przeglądzanie strony\\nPodstrona Battle.net jest dostępna w dwóch wersjach językowych: angielskim i pochińskim.\\nStrona posiada wyświetlecie aktualizacji, które informują o nowych transakcjach gry, aktualizacjach gier i informują o planowanych wydarzeniach.\\n\\n### Nota o grych\\nBattle.net to platforma rozgrywek komputerowych. Obsługuje wiele gier Blizzard Entertainment, takich jak World of Warcraft, StarCraft II, Diablo III oraz Hearthstone.\\nJasne jest to tylko kilka przykładów gier obsługiwanych przez Battle.net.\\n\\n### Informacje o transakcjach\\nMożesz kupić i sprzedawać gry poprzez platformę Battle.net Store. Zapisujemy się na naszą newsletter, aby otrzymywać informacje o specjalnych promocjach i specjalnych doświadczeniach.\\n\\n### Aktualne nagłoszenia\\nObecnie jest ogłoszony wybuch nowego sezonu w World of Warcraft - \"Dragonflight\".'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summarize(\"https://wp.pl\")\n",
    "summarize(\"https://eu.shop.battle.net/pl-pl#optLogin=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "862c858d-3dd5-42b5-be82-1b39e596d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize (url)\n",
    "    display (Markdown (summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b92a0d50-f6e5-435d-90eb-90cf1e6f3b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Sumary Battle.net\n",
       "\n",
       "### Historia i Produkty\n",
       "\n",
       "* StronaBattle.net pochodzi z 2007 roku, kiedy Blizzard Entertainment ogłosiło swoją decyzję o utworzeniu dedicated platformy dla graczy wideo i gier komputerowych.\n",
       "* Platforma oferuje wiele popularnych gier, takich jak Warcraft III: Reign of Chaos, StarCraft II, Diablo III i Overwatch.\n",
       "* Ponadto na stronie można znaleźć informacje o aktualnie rozwijanych produktach, takich jak World of Warcraft i Heroes of the Storm.\n",
       "\n",
       "### Aktualne Wiadomości\n",
       "\n",
       "Z powodu braku bieżących wiadomości nie jest möglich podać ogólnej listy aktuality z Battle.net."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display_summary(\"https://wp.pl\")\n",
    "display_summary(\"https://eu.shop.battle.net/pl-pl#optLogin=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897928f-a5ac-4e84-a593-3f718c187a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
